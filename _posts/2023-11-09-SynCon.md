---
layout: distill
title: Contrastive Learning with Dynamically Weighted Synthetic Images
description: Final Project Proposal
date: 2023-11-09
htmlwidgets: true

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Fiona Cai
    url: 
    affiliations:
      name: MIT


# must be the exact same name as your blogpost
bibliography: 2023-11-09-SynCon.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Final Project Proposal



# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Final Project Proposal

Powerful text-to-image generation models enable the synthesis of high-quality images from textual descriptions. Recent advancements have led to research that leverages generative AI models to create synthetic data to provide additional support for various learning tasks
<d-cite key="voetman2023big, ruiz2023dreambooth, zhang2023prompt, ramesh2021zeroshot"></d-cite>. When properly configured, these models can create synthetic images that enable self-supervised methods to match or even exceed performance for challenging discriminative tasks relative to training on real images <d-cite key="tian2023stablerep, azizi2023synthetic"></d-cite>. Synthetic images are also highly beneficial in few-shot learning settings since they can be used as support images to augment the training data set distribution and improve model performance on downstream tasks <d-cite key="sariyildiz2023fake, he2023synthetic"></d-cite>. %They can also address the common challenges of data scar city and diversity in certain real-world domains <d-cite key="udandarao2023susx"></d-cite>.

Despite these advancements, challenges persist, particularly regarding the variation in quality of generated synthetic images across diverse domains and datasets. Generative AI models often fail to capture fine-grain details, especially in complex scenarios that require additional context <d-cite key="marcus2022preliminary"></d-cite>. Synthetic support images do not always completely capture the domain characteristics of the true data distribution, and while they may improve performance in certain classification scenarios, they can result in less accurate models in other domains <d-cite key="udandarao2023susx, jahanian2022generative"></d-cite>. This drawback makes it challenging to effectively integrate synthetic support set samples in the learning process, especially when used for difficult vision tasks. Existing methods often treat synthetic images as if they were as informative as real images in the training process <d-cite key="ren2017crossdomain, azizi2023synthetic"></d-cite>. We hypothesize that it would be better to adjust the utilization of synthetic images based on their quality and relevance to the corresponding real data. We explore this issue by developing a method that combines supervised contrastive learning (SupCon) <d-cite key="khosla2021supervised"></d-cite> with weighted representation learning. The weighting parameter modulates the contribution of synthetic images to the learned embedding.  

For my final project, I am proposing Contrastive Learning with Dynamically Weighted Synthetic Images (SynCon, name pending?) through a novel contrastive loss function designed to learn uncertainty weights for synthetic images. These weights would be dynamically learned and would reflect the quality and contribution of synthetic images to class representation. Extending the supervised contrastive method, we consider many positive examples per anchor, where positive examples now include both real and synthetic images per class. For each class, the optimization pulls together normalized embeddings of real images of the same class in the usual way. It also pulls together real and synthetic images from the same class but scaled by a weighting value. I will experiment with two main methods of learning the weighting hyperparameter per synthetic image class, specifically regular SGD and using a hypernetwork <d-cite key="ha2016hypernetworks"></d-cite>. I will test the loss function by using DALL-E to generate synthetic images per class and will focus on popular vision datasets such as Flowers102, FGVCAircraft, Birdsnap for testing and evaluation. I will use downstream classification accuracy as a metric to evaluate the proposed loss function and compare to baselines SupCon with and without synthetic images. This is the proposed loss function equation: 

$$
\begin{equation} \label{eq:3}
    \mathcal{L}_{SynCon} = 
\sum_{i\in S(i)}\Bigl(\frac{-1}{|P(i)|}\sum_{p\in P(i)} log\frac{exp(\frac{v_i^Tv_p}{\tau})}{\sum_{a\in A(i)}exp(\frac{v_i^Tv_a}{\tau})} + \frac{-w}{|U(i)|}\sum_{u\in U(i)}log \frac{exp(\frac{v_i^Tv_u}{\tau})}{\sum_{a\in A(i)}exp(\frac{v_i^Tv_a}{\tau})}\Bigl)
\end{equation}
$$

Here, we consider the set $S$ as all the real images in the multi-view batch such that $S \subseteq X$. Then for each anchor image $i \in S(i)$, $P(i)$ refers to the set of all indices of positive pairs from the same class that are real images so $P(i) \subseteq X$. The left term of the outer summation is exactly identical to the SupCon loss function but applied only to the positive examples that are real images.

The right term introduces $w \in [0, 1]$, a weighting hyperparameter that provides a mechanism to modulate the influence of the embeddings of support set data in the learning process. The current loss setup uses the same weighting per class. $U(i)$ refers to the set of all indices of inputs that are in the same class as anchor $i$ but are support set images. Finally, the set $A(i)$ remains as described above. Effectively, each normal input anchor $i$ contributes to the UniCon loss through the sum of the SupCon loss and a weighted sum of all the similarities between the anchor and its corresponding support set images.



